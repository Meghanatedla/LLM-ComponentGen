{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from CodeQwenHf import CodeQwen\n",
    "from ArtigenzCoder import ArtigenzCoder\n",
    "import time\n",
    "import json\n",
    "from tqdm.auto import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://qwen-codeqwen1-5-7b-chat-demo.hf.space âœ”\n"
     ]
    }
   ],
   "source": [
    "codeqwen = CodeQwen()\n",
    "# artigenz_coder = ArtigenzCoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_config(config: dict):\n",
    "    required_keys = [\n",
    "    \"language\", \"summarize_codebase\", \"codebase_readme_path\", \"files_to_summarize_paths\", \"codebase_summary_prompt_template\", \"codebase_summary_prompt_save_path\", \"codebase_summary_save_path\", \"function_description_prompt_template\", \"function_description_prompt_save_path\", \"function_description_save_path\",  \"function_generation_prompt_template_type1\", \"function_generation_prompt_type1_save_path\", \"function_generation_prompt_template_type2\", \"function_generation_prompt_type2_save_path\", \"function_generation_prompt_template_type3\", \"function_generation_prompt_type3_save_path\", \"chosen_function_path\", \"chosen_function\", \"original_function_save_path\", \"example_function_description1\", \"example_function_code1\", \"example_function_description2\", \"example_function_code2\", \"generated_function_type1_save_dir\", \"generated_function_type2_save_dir\", \"generated_function_type3_save_dir\", \"run_codebleu\", \"codebleu_type1_save_dir\", \"codebleu_type2_save_dir\", \"codebleu_type3_save_dir\"     \n",
    "    ]\n",
    "    \n",
    "    for key in required_keys:\n",
    "        if key not in config:\n",
    "            raise ValueError(f\"Missing required key: {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_confs_file = \"localllm_remaining_confs.txt\"\n",
    "with open(remaining_confs_file, 'r') as f:\n",
    "    remaining_confs_paths = f.read().splitlines()\n",
    "\n",
    "remaining_confs = []\n",
    "for cf_path in remaining_confs_paths:\n",
    "    conf = json.load(open(cf_path))\n",
    "    try:\n",
    "        validate_config(conf)\n",
    "        remaining_confs.append(conf)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {cf_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_confs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_func_generation_prompts(config):\n",
    "    with open(config['function_generation_prompt_type1_save_path'], 'r') as f:\n",
    "        function_generation_prompt_type1 = f.read()\n",
    "    \n",
    "    with open(config['function_generation_prompt_type2_save_path'], 'r') as f:\n",
    "        function_generation_prompt_type2 = f.read()\n",
    "\n",
    "    function_generation_prompt_type3 = \"\"\n",
    "    if config[\"function_generation_prompt_type3_save_path\"] != \"\":\n",
    "        with open(config['function_generation_prompt_type3_save_path'], 'r') as f:\n",
    "            function_generation_prompt_type3 = f.read()\n",
    "\n",
    "    return function_generation_prompt_type1, function_generation_prompt_type2, function_generation_prompt_type3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    # \"GPT-3_5-Turbo\": OpenAIModel(\"gpt-3.5-turbo\"), \n",
    "    # \"GPT-4\": OpenAIModel(\"gpt-4\"), \n",
    "    # \"DeepSeek-Coder-V2\": DeepSeek(), \n",
    "    \"CodeQwen1_5-7B-Chat\": codeqwen, \n",
    "    # \"Artigenz-Coder-DS-6_7B\": artigenz_coder\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c8f1156df84aa5b4004afae19d2369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Remaining configurations:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Running prompt type type1\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type2\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type3\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "-----------------------------------\n",
      "Running prompt type type1\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type2\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type3\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "-----------------------------------\n",
      "Running prompt type type1\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type2\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type3\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "-----------------------------------\n",
      "Running prompt type type1\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type2\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Skipping function generation prompt type 3\n",
      "-----------------------------------\n",
      "Running prompt type type1\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type2\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type3\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "-----------------------------------\n",
      "Running prompt type type1\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type2\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type3\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "-----------------------------------\n",
      "Running prompt type type1\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type2\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type3\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "-----------------------------------\n",
      "Running prompt type type1\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type2\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type3\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "-----------------------------------\n",
      "Running prompt type type1\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type2\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type3\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "-----------------------------------\n",
      "Running prompt type type1\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type2\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n",
      "Running prompt type type3\n",
      "Running CodeQwen1_5-7B-Chat model\n",
      "Generated function 1\n"
     ]
    }
   ],
   "source": [
    "for conf in tqdm(remaining_confs, desc=\"Remaining configurations\"):\n",
    "    print(\"-----------------------------------\")\n",
    "    function_generation_prompt_type1, function_generation_prompt_type2, function_generation_prompt_type3 = load_func_generation_prompts(conf)\n",
    "    function_generation_prompts = {\n",
    "        \"type1\": function_generation_prompt_type1,\n",
    "        \"type2\": function_generation_prompt_type2,\n",
    "        \"type3\": function_generation_prompt_type3\n",
    "    }\n",
    "\n",
    "    generated_function_save_dirs = {\n",
    "        \"type1\": conf['generated_function_type1_save_dir'],\n",
    "        \"type2\": conf['generated_function_type2_save_dir'],\n",
    "        \"type3\": conf['generated_function_type3_save_dir']\n",
    "    }\n",
    "\n",
    "    model_names = [\"GPT-3_5-Turbo\", \"GPT-4\", \"DeepSeek-Coder-V2\", \"CodeQwen1_5-7B-Chat\", \"Artigenz-Coder-DS-6_7B\"]\n",
    "    prompt_types = [\"type1\", \"type2\", \"type3\"]\n",
    "\n",
    "    generated_function_save_paths = {t: {m: [] for m in model_names} for t in prompt_types}\n",
    "    generation_count = 1\n",
    "\n",
    "\n",
    "    for prompt_type in [\"type1\", \"type2\", \"type3\"]:\n",
    "    # for prompt_type in [\"type3\"]:\n",
    "        if prompt_type == \"type3\" and conf[\"generated_function_type3_save_dir\"] == \"\":\n",
    "            print(\"Skipping function generation prompt type 3\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Running prompt type {prompt_type}\")\n",
    "\n",
    "        for model_name, model in model_dict.items():\n",
    "            print(f\"Running {model_name} model\")\n",
    "\n",
    "            for i in range(1, generation_count + 1):\n",
    "\n",
    "                generated_function = model.generate(function_generation_prompts[prompt_type])\n",
    "\n",
    "                filename = conf[\"chosen_function\"].split(\".\")\n",
    "                generated_function_save_filename = f\"{filename[0]}_{i}.{filename[1]}\"\n",
    "                generated_function_save_path = f\"{generated_function_save_dirs[prompt_type]}/{model_name}/GENERATED-{generated_function_save_filename}\"\n",
    "                model.write_to_file(generated_function_save_path)\n",
    "                \n",
    "                generated_function_save_paths[prompt_type][model_name].append(generated_function_save_path)\n",
    "                print(f\"Generated function {i}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
